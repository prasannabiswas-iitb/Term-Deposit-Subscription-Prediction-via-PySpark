{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row, Column\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read a csv file\n",
    "df = spark.read.csv('bank.csv',header = True, inferSchema = True)\n",
    "\n",
    "# see the default schema of the dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Custom Schema\n",
    "Spark by default treats every data as string. Now we customize the datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Batsman: integer (nullable = true)\n",
      " |-- Batsman_Name: string (nullable = true)\n",
      " |-- Bowler: integer (nullable = true)\n",
      " |-- Bowler_Name: string (nullable = true)\n",
      " |-- Commentary: string (nullable = true)\n",
      " |-- Detail: string (nullable = true)\n",
      " |-- Dismissed: integer (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Isball: boolean (nullable = true)\n",
      " |-- Isboundary: binary (nullable = true)\n",
      " |-- Iswicket: binary (nullable = true)\n",
      " |-- Over: double (nullable = true)\n",
      " |-- Runs: integer (nullable = true)\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as tp\n",
    "\n",
    "# define the schema\n",
    "my_schema = tp.StructType([\n",
    "    tp.StructField(name= 'Batsman',      dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Batsman_Name', dataType= tp.StringType(),    nullable= True),\n",
    "    tp.StructField(name= 'Bowler',       dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Bowler_Name',  dataType= tp.StringType(),    nullable= True),\n",
    "    tp.StructField(name= 'Commentary',   dataType= tp.StringType(),    nullable= True),\n",
    "    tp.StructField(name= 'Detail',       dataType= tp.StringType(),    nullable= True),\n",
    "    tp.StructField(name= 'Dismissed',    dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Id',           dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Isball',       dataType= tp.BooleanType(),   nullable= True),\n",
    "    tp.StructField(name= 'Isboundary',   dataType= tp.BinaryType(),   nullable= True),\n",
    "    tp.StructField(name= 'Iswicket',     dataType= tp.BinaryType(),   nullable= True),\n",
    "    tp.StructField(name= 'Over',         dataType= tp.DoubleType(),    nullable= True),\n",
    "    tp.StructField(name= 'Runs',         dataType= tp.IntegerType(),   nullable= True),\n",
    "    tp.StructField(name= 'Timestamp',    dataType= tp.TimestampType(), nullable= True)    \n",
    "])\n",
    "\n",
    "# read the data again with the defined schema\n",
    "my_data = spark.read.csv('ind-ban-comment.csv',schema= my_schema,header= True)\n",
    "\n",
    "# print the schema\n",
    "my_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop the columns that are not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Batsman_Name',\n",
       " 'Bowler_Name',\n",
       " 'Commentary',\n",
       " 'Detail',\n",
       " 'Dismissed',\n",
       " 'Isball',\n",
       " 'Isboundary',\n",
       " 'Iswicket',\n",
       " 'Over',\n",
       " 'Runs',\n",
       " 'Timestamp']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = my_data.drop(*['Batsman', 'Bowler', 'Id'])\n",
    "my_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dimensions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(605, 11)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(my_data.count() , len(my_data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the summary of the numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|              Runs|\n",
      "+-------+------------------+\n",
      "|  count|               605|\n",
      "|   mean|0.9917355371900827|\n",
      "| stddev| 1.342725481259329|\n",
      "|    min|                 0|\n",
      "|    max|                 6|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_data.select('Isball', 'Isboundary', 'Runs').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Batsman_Name: bigint, Bowler_Name: bigint, Commentary: bigint, Detail: bigint, Dismissed: bigint, Isball: bigint, Isboundary: bigint, Iswicket: bigint, Over: bigint, Runs: bigint, Timestamp: bigint]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "data_agg = my_data.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in my_data.columns])\n",
    "data_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value counts of Batsman_Name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|      Batsman_Name|count|\n",
      "+------------------+-----+\n",
      "|     Soumya Sarkar|   39|\n",
      "|  Mashrafe Mortaza|    5|\n",
      "|   Shakib Al Hasan|   75|\n",
      "|   Mushfiqur Rahim|   23|\n",
      "|Mohammad Saifuddin|   42|\n",
      "|         Liton Das|   24|\n",
      "|      Rishabh Pant|   43|\n",
      "|    Mohammed Shami|    2|\n",
      "|       Tamim Iqbal|   31|\n",
      "|     Hardik Pandya|    2|\n",
      "|          KL Rahul|   93|\n",
      "| Bhuvneshwar Kumar|    4|\n",
      "|     Rubel Hossain|   11|\n",
      "|      Rohit Sharma|   94|\n",
      "|    Dinesh Karthik|    9|\n",
      "|       Virat Kohli|   27|\n",
      "|          MS Dhoni|   33|\n",
      "|     Sabbir Rahman|   40|\n",
      "|  Mosaddek Hossain|    7|\n",
      "| Mustafizur Rahman|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_data.groupBy('Batsman_Name').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+------------------+------------+\n",
      "|     Batsman_Name|Batsman_Index|       Bowler_Name|Bowler_Index|\n",
      "+-----------------+-------------+------------------+------------+\n",
      "|   Mohammed Shami|         18.0| Mustafizur Rahman|         0.0|\n",
      "|Bhuvneshwar Kumar|         16.0| Mustafizur Rahman|         0.0|\n",
      "|   Mohammed Shami|         18.0| Mustafizur Rahman|         0.0|\n",
      "|Bhuvneshwar Kumar|         16.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
      "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
      "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
      "+-----------------+-------------+------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# create object of StringIndexer class and specify input and output column\n",
    "SI_batsman = StringIndexer(inputCol='Batsman_Name',outputCol='Batsman_Index')\n",
    "SI_bowler = StringIndexer(inputCol='Bowler_Name',outputCol='Bowler_Index')\n",
    "\n",
    "# transform the data\n",
    "my_data = SI_batsman.fit(my_data).transform(my_data)\n",
    "my_data = SI_bowler.fit(my_data).transform(my_data)\n",
    "\n",
    "# view the transformed data\n",
    "my_data.select('Batsman_Name', 'Batsman_Index', 'Bowler_Name', 'Bowler_Index').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
